<h2>DESCRIPTION</h2>

<p>Several commonly used classifiers and regressors are exposed in <em>r.learn.train</em> and the choice of classifier
	is set using the <em>model_name</em> parameter. For more details relating to the classifiers, refer to the <a
		href="http://scikit-learn.org/stable/">scikit learn documentation.</a> The following classification and
	regression methods are available:</p>

<ul>
	<li><em>LogisticRegression</em> and <em>LinearRegression</em> represent linear models for classification or
		regression. <em>SGDClassifier</em> and <em>SGDRegressor</em> are also linear models for classification and
		regression, but they use stochastic gradient descent for optimization</li>

	<li><em>LinearDiscriminantAnalysis</em> and <em>QuadraticDiscriminantAnalysis</em> are classifiers with linear and
		quadratic decision surfaces. These classifiers do not take any parameters and are inherently multiclass. They
		can only be used for classification.</li>

	<li><em>KNeighborsClassifier</em> classifies samples based on closest distance to a predefined number of training
		samples. Two hyperparameters are exposed, with <em>n_neighbors</em> governing the number of neighbors to use to
		decide the prediction label, and <em>weights</em> specifying whether these neighbors should have equal weights
		or whether they should be inversely weighted by their distance.</li>

	<li><em>GaussianNB</em> is the Gaussian Naive Bayes algorithm and can be used for classification only. Naive Bayes
		is a supervised learning algorithm based on applying Bayes theorem with the naive assumption of independence
		between every pair of features. This classifier does not take any parameters.</li>

	<li>The <em>DecisionTreeClassifier</em> and <em>DecisionTreeRegressor</em> map observations to a response variable
		using a hierarchy of splits and branches. The terminus of these branches, termed leaves, represent the
		prediction of the response variable. Decision trees are non-parametric and can model non-linear relationships
		between a response and predictor variables, and are insensitive the scaling of the predictors.</li>

	<li>The <em>RandomForestsClassifier</em> and <em>RandomForestsRegressor</em> represent ensemble classification and
		regression tree methods. Random forests overcome some of the disadvantages of single decision trees by
		constructing an ensemble of uncorrelated trees. Each tree is constructed from a random subsample of the training
		data and only a random subset of the predictors based on <em>max_features</em> is made available during each
		node split. Each tree produces a prediction probability and the final classification result is obtained by
		averaging of the prediction probabilities across all of the trees. The <em>ExtraTreesClassifier</em> is a
		variant on random forests where during each node split, the splitting rule that is selected is based on the best
		of a collection of randomly-geneated thresholds that were assigned to the features.</li>

	<li>The <em>GradientBoostingClassifier</em> and <em>GradientBoostingRegressor</em> also represent ensemble
		tree-based methods. However, in a boosted model the learning processes is additive in a forward step-wise
		fashion whereby <i>n_estimators</i> are fit during each model step, and each model step is designed to better
		fit samples that are not currently well predicted by the previous step. This incrementally improves the
		performance of the entire model ensemble by fitting to the model residuals. The
		<em>HistGradientBoostingClassifier</em> and <em>HistGradientBoostingRegressor</em> use the new scikit learn
		multi-threaded implementations.</li>

	<li>The <em>SVC</em> and <em>SVR</em> options are C-Support Vector classifiers or regressors. Only a linear kernel
		is supported because non-linear kernels using scikit learn for typical remote sensing and spatial analysis
		datasets which consist of large numbers of samples are too slow to be practical. This classifier can still be
		slow for large datasets that include &gt 10000 training samples.</li>

	<li>The <em>MLPClassifier</em> and <em>MLPRegressor</em> options provide access to a multi-layer perceptron
		classifier or regressor.</li>
</ul>

<p>The Classifier parameters tab provides access to the most pertinent parameters that affect the previously described
	algorithms. The scikit-learn classifier defaults are generally supplied, and some of these parameters can be tuning
	using a grid-search by inputting multiple parameter settings as a comma-separated list. This tuning can also be
	accomplished simultaneously with nested cross-validation by also settings the <em>cv</em> option to &gt 1. The
	parameters consist of:</p>

<ul>
	<li><em>alpha</em> is the constant used to multiply the regularization term for the <em>SGDClassifier</em> and
		<em>SGDRegressor</em> algorithm, as well as the <em>MLRClassifier</em> and <em>MLPRegressor</em>.</li>

	<li><em>l1_ratio</em> represents the elastic net mixing ratio between l1 and l2 regularization. Applies to
		the <em>SGDClassifier</em> and <em>SGDRegressor</em> options.</li>

	<li><em>c</em> is the inverse of the regularization strength, which is when a penalty is applied to avoid
		overfitting. <em>c</em> applies to the <em>LogisticRegression</em> and <em>SVC/SVR</em> models.</li>

	<li><em>epsilon</em> represents the width of the margin that is used to maximize the number of fitted
		observations within. Applies only to the <em>SVR</em> model.</li>

	<li><em>n_estimators</em> represents the number of trees in Random Forest model, and the number of trees used in
		each model step during Gradient Boosting. For random forests, a larger number of trees will never adversely
		affect accuracy although this is at the expensive of computational performance. In contrast, Gradient boosting
		will start to overfit if <em>n_estimators</em> is too high, which will reduce model accuracy.</li>

	<li><em>max_features</em> controls the number of variables that are allowed to be chosen from at each node split in
		the tree-based models, and can be considered to control the degree of correlation between the trees in ensemble
		tree methods.</li>

	<li><em>min_samples_split</em> and <em>min_samples_leaf</em> control the number of samples required to split a node
		or form a leaf node, respectively.</li>

	<li>The <em>learning_rate</em> and <em>subsample</em> parameters apply only to Gradient Boosting and XGBClassifier
		or XGBRegressor. <em>learning_rate</em> shrinks the contribution of each tree, and <em>subsample</em> is the
		fraction of randomly selected samples for each tree. A lower <em>learning_rate</em> always improves accuracy in
		gradient boosting but will require a much larger <em>n_estimators</em> setting which will lower computational
		performance.</li>

	<li><em>hidden_units</em> specifies the number of neurons within each hidden layer when using the
		<em>MLPClassifier</em>
		or <em>MLPRegressor</em> methods. The syntax for this parameter is (100;100) for 100 neurons in two hidden
		layers,
		or for example (300;100;50) for 300 neurons in the first hidden layer, 100 neurons in the second layer, and
		50 neurons in the third. Hyperparameter tuning of the number of hidden layers and the number of neurons in
		each layer can be performed by specifying multiple comma-separated values, e.g. (100;100),(200;200).</li>
</ul>

<h3>Feature Importances</h3>

<p>In addition to model fitting and prediction, feature importances can be generated using the <em>-f</em> flag. The
	feature importances method uses a permutation-based method can be applied to all of the estimators. The feature
	importances represent the average decrease in performance of each variable when permuted. For binary
	classifications, the AUC is used as the metric. Multiclass classifications use accuracy, and regressions use R2.</p>

<h3>Cross Validation</h3>

<p>Cross validation can be performed by setting the <em>cv</em> parameters to &gt 1. Cross-validation is performed using
	stratified kfolds, and multiple global and per-class accuracy measures are produced depending on whether the
	response variable is binary or multiclass, or the classifier is for regression or classification. Cross-validation
	can also be performed in groups by supplying a raster containing the group_ids of the partitions using the
	<em>group_raster</em> option. In this case, training samples with the same group id as set by the group_raster will
	never be split between training and test partitions during cross-validation. This can reduce problems with overly
	optimistic cross-validation scores if the training data are strongly spatially correlated, i.e. the training data
	represent rasterized polygons.</p>

<h3>Preprocessing</h3>

<p>Although tree-based classifiers are insensitive to the scaling of the input data, other classifiers such as linear
	models may not perform optimally if some predictors have variances that are orders of magnitude larger than others.
	The <em>-s</em> flag adds a standardization preprocessing step to the classification and prediction to reduce this
	effect. Additionally, most of the classifiers do not perform well if there is a large class imbalance in the
	training data. Using the <em>-b</em> flag balances the training data by weighting of the minority classes relative
	to the majority class. This does not apply to the Naive Bayes or LinearDiscriminantAnalysis classifiers.</p>

<p>Non-ordinal, categorical predictors are not specifically recognized by scikit-learn. Some classifiers are not very
	sensitive to this (i.e. decision trees) but generally, categorical predictors need to be converted to a suite of
	binary using onehot encoding (i.e. where each value in a categorical raster is parsed into a separate binary grid).
	Entering the names of the categorical rasters in the imagery group in the <em>categorymaps</em> option will cause
	onehot encoding to be performed on the fly during training and prediction.</p>

<p>For convenience when repeatly training models on the same data, the training data can be saved to a csv file using
	the <em>save_training</em> option. This data can then be loaded into subsequent classification runs, saving time by
	avoiding the need to repeatedly query the predictors.</p>

<h2>NOTES</h2>

<p>Many of the classifiers involve a random process which can causes a small amount of variation in the classification
	results, out-of-bag error, and feature importances. To enable reproducible results, a seed is supplied to the
	classifier. This can be changed using the <em>randst</em> parameter.</p>

<h2>EXAMPLE</h2>

<p>Here we are going to use the GRASS GIS sample North Carolina data set as a basis to perform a landsat classification.
	We are going to classify a Landsat 7 scene from 2000, using training information from an older (1996) land cover
	dataset.</p>

<p>Landsat 7 (2000) bands 7,4,2 color composite example:</p>
<center>
	<img src="lsat7_2000_b742.png" alt="Landsat 7 (2000) bands 7,4,2 color composite example">
</center>

<p>Note that this example must be run in the "landsat" mapset of the North Carolina sample data set location.</p>

<p>First, we are going to generate some training pixels from an older (1996) land cover classification:</p>
<div class="code">
	<pre>
g.region raster=landclass96 -p
r.random input=landclass96 npoints=1000 raster=landclass96_roi
</pre>
</div>

<p>Then we can use these training pixels to perform a classification on the more recently obtained landsat 7 image:</p>
<div class="code">
	<pre>
r.learn.train group=lsat7_2000 training_map=landclass96_roi \
  model_name=RandomForestClassifier n_estimators=500 save_model=rf_model.gz

r.learn.predict group=lsat7_2000 load_model=rf_model.gz output=rf_classification

# copy category labels from landclass training map to result
r.category rf_classification raster=landclass96_roi
# copy color scheme from landclass training map to result
r.colors rf_classification raster=landclass96_roi
r.category rf_classification
</pre>
</div>

<p>Random forest classification result:</p>
<center>
	<img src="rfclassification.png" alt="Random forest classification result">
</center>

<h2>ACKNOWLEDGEMENTS</h2>
<p>Thanks for Paulo van Breugel and Vaclav Petras for testing.</p>

<h2>REFERENCES</h2>

<p>Brenning, A. 2012. Spatial cross-validation and bootstrap for the assessment of prediction rules in remote sensing:
	the R package 'sperrorest'. 2012 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 23-27 July
	2012, p. 5372-5375.</p>

<p>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.</p>

<h2>AUTHOR</h2>

Steven Pawley
<p><em>Last changed: $Date: 2019-02-08 15:41:00 -0600 (Fri, 08 Feb 2019) $</em></p>